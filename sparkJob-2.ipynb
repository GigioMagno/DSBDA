{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0709be94-8d86-413f-97ee-eec321f59309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, regexp_replace, sum, when, count, round as sparkrnd, max as sparkmax, min as sparkmin, year, avg as sparkavg, stddev as sparkstd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7f0f1e-5f10-4f03-9ad8-a858e5c679cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 13:18:17 WARN Utils: Your hostname, dsbda-vm resolves to a loopback address: 127.0.1.1; using 192.168.64.2 instead (on interface enp0s1)\n",
      "24/06/24 13:18:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/24 13:18:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295b1427-6631-4731-9f3d-bd98ef8c6184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+----------------------+------------------+------------------+-----------------+--------------+-----+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------------+\n",
      "|   id|                name|host_name|neighbourhood_cleansed|          latitude|         longitude|    property_type|     room_type|price|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|reviews_per_month|\n",
      "+-----+--------------------+---------+----------------------+------------------+------------------+-----------------+--------------+-----+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------------+\n",
      "| 2818|Quiet Garden View...|   Daniel|  Oostelijk Havenge...|52.365754513876176| 4.941419235184398|        Apartment|  Private room| 59.0|                  97|                    10|                       10|                   10|                         10|                     9|                 10|              2.1|\n",
      "| 3209|Quiet apt near ce...|  Maartje|            Westerpark| 52.39022505041117| 4.873924094742859|        Apartment|Entire homeapt|160.0|                  96|                    10|                        9|                   10|                         10|                     9|                  9|             1.03|\n",
      "|20168|100CentreStudio 1...|     Alex|           CentrumOost|52.365087026809164|4.8935410078410095|        Townhouse|Entire homeapt| 80.0|                  87|                     9|                       10|                    9|                          9|                    10|                  9|             2.18|\n",
      "|25428|Lovely apt in Cit...|     Joan|           CentrumWest| 52.37311440038617| 4.883668196205626|        Apartment|Entire homeapt|125.0|                 100|                    10|                       10|                   10|                         10|                    10|                 10|             0.09|\n",
      "|27886|Romantic stylish ...|     Flip|           CentrumWest| 52.38672731612469| 4.892078070089338|        Houseboat|  Private room|150.0|                  99|                    10|                       10|                   10|                         10|                    10|                 10|             2.03|\n",
      "|28658|Cosy guest room n...|  Michele|         Bos en Lommer| 52.37534217754822| 4.857289350245441|        Apartment|  Private room| 65.0|                  93|                     9|                       10|                   10|                          9|                     9|                  9|             4.16|\n",
      "|28871|Comfortable doubl...|    Edwin|           CentrumWest| 52.36718680818601| 4.890917797129656|        Apartment|  Private room| 75.0|                  97|                    10|                       10|                   10|                         10|                    10|                 10|             2.13|\n",
      "|29051|Comfortable singl...|    Edwin|           CentrumWest| 52.36772541044574| 4.891511737154662|        Apartment|  Private room| 55.0|                  95|                    10|                       10|                   10|                         10|                    10|                 10|             4.07|\n",
      "|31080|2story apartment ...|   Nienke|                  Zuid|52.351320541867395|  4.84838289937379|        Apartment|Entire homeapt|219.0|                  95|                     9|                       10|                   10|                         10|                     9|                  9|             0.36|\n",
      "|41125|Amsterdam Center ...|    Fatih|           CentrumWest|  52.3789147700269|4.8832050564993645|        Apartment|Entire homeapt|180.0|                  95|                    10|                        9|                   10|                         10|                    10|                  9|             0.78|\n",
      "|42970|Comfortable roomP...|    Micha|           CentrumWest| 52.36781447589647| 4.890012023439633|Bed and breakfast|  Private room|159.0|                  98|                    10|                       10|                   10|                         10|                    10|                  9|             4.24|\n",
      "|43980|View into park  m...|       Ym|                  Zuid|  52.3574562838326| 4.861242085387215|        Apartment|Entire homeapt|100.0|                  80|                     8|                        7|                    9|                          9|                     9|                  8|             0.62|\n",
      "|44129|Luxury design wit...|    Tanya|           CentrumWest|52.380711229831924| 4.886104318098604|        Apartment|Entire homeapt|250.0|                  98|                    10|                       10|                   10|                         10|                    10|                  9|             1.74|\n",
      "|44391|Amsterdam Centre ...|      Jan|           CentrumOost| 52.37016998640934| 4.914377826765493|        Apartment|Entire homeapt|200.0|                  94|                    10|                        9|                   10|                         10|                     9|                  9|             0.31|\n",
      "|45246|Cozy Flat in the ...|   George|            Westerpark| 52.37767639657083| 4.876178648987073|        Apartment|Entire homeapt|155.0|                  92|                     9|                        9|                   10|                         10|                    10|                  9|             0.79|\n",
      "|46386|Cozy loft in cent...|    Joost|  De Pijp  Rivieren...| 52.35247025928299| 4.908254440320654|        Apartment|Entire homeapt|150.0|                  93|                    10|                        9|                    9|                          9|                     8|                  8|             0.03|\n",
      "|47061|Charming apartmen...|     Ivar|  De Baarsjes  OudWest| 52.36798609885809| 4.874469331907506|        Apartment|Entire homeapt|140.0|                  96|                    10|                        9|                   10|                         10|                    10|                  9|             1.67|\n",
      "|48076|Amsterdam Central...| Franklin|           CentrumWest|  52.3804200178011| 4.894534735097551|        Apartment|Entire homeapt|350.0|                  97|                    10|                       10|                   10|                         10|                    10|                  9|             1.82|\n",
      "|49790|Luxurous Houseboa...|    Klaas|  De Baarsjes  OudWest|52.362663786120464| 4.861031740012395|             Boat|Entire homeapt|225.0|                  99|                    10|                       10|                   10|                         10|                    10|                  9|             1.06|\n",
      "|50515|Family Home No dr...|    Karin|         Bos en Lommer| 52.37727239832183| 4.839252562940868|        Townhouse|Entire homeapt|120.0|                  91|                     9|                        9|                   10|                          9|                     8|                  9|             0.21|\n",
      "+-----+--------------------+---------+----------------------+------------------+------------------+-----------------+--------------+-----+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/ubuntu/DATASET/listings_details.csv\"\n",
    "file_path_out = \"/home/ubuntu/DATASET/listings_details_select\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\",\", quote= '\"', escape= '\"', multiLine=True, mode=\"DROPMALFORMED\")\n",
    "df_cleaned = df.withColumn(\"price\", regexp_replace(col(\"price\"), r\"[\\$,]\", \"\").cast(\"double\"))\n",
    "for col_name in df_cleaned.columns:\n",
    "        df_cleaned= df_cleaned.withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9 .]\", \"\"))\n",
    "        #df_cleaned = df_cleaned.withColumn(col_name, regexp_replace(col(col_name), \"[\\n\\t\\r]\", \"\"))\n",
    "df_filtered = df_cleaned.filter( col(\"id\").cast(IntegerType()).isNotNull()\n",
    ")\n",
    "\n",
    "#df_filtered.printSchema()\n",
    "df_filtered = df_filtered.select(\"id\", \"name\", \"host_name\", \"neighbourhood_cleansed\", \"latitude\", \"longitude\", \n",
    "                                 \"property_type\", \"room_type\", \"price\", \"review_scores_rating\", \"review_scores_accuracy\", \n",
    "                                 \"review_scores_cleanliness\", \"review_scores_checkin\", \"review_scores_communication\", \n",
    "                                 \"review_scores_location\", \"review_scores_value\", \"reviews_per_month\")\n",
    "df_filtered.show()\n",
    "\n",
    "df_filtered.write.csv(file_path_out, header=False)\n",
    "\n",
    "# The following lines generate an error because it says that no namenode is running even if\n",
    "#I run the command jps and I can see the running namenodes\n",
    "\n",
    "#hdfs_path = \"hdfs://localhost:54310/user/ubuntu/ciao/reviews_details_cleaned.csv\"\n",
    "#df_filtered.write.csv(hdfs_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ea067c-c48b-4ed1-b042-a1ea76fc00af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "+----------+-----+----------+-----------+-------------+--------------------+\n",
      "|listing_id|   id|      date|reviewer_id|reviewer_name|            comments|\n",
      "+----------+-----+----------+-----------+-------------+--------------------+\n",
      "|      2818| 1191|2009-03-30|      10952|          Lam|Daniel is really ...|\n",
      "|      2818| 1771|2009-04-24|      12798|        Alice|Daniel is the mos...|\n",
      "|      2818| 1989|2009-05-03|      11869|      Natalja|We had such a gre...|\n",
      "|      2818| 2797|2009-05-18|      14064|      Enrique|Very professional...|\n",
      "|      2818| 3151|2009-05-25|      17977|      Sherwin|Daniel is highly ...|\n",
      "|      2818| 4748|2009-06-29|      20192|          Jie|Daniel was a grea...|\n",
      "|      2818| 5202|2009-07-07|      23055|      Vanessa|Daniele is an ama...|\n",
      "|      2818| 9131|2009-09-06|      26343|        Katja|You cant have a n...|\n",
      "|      2818|12103|2009-10-01|      40999|     MarieEve|Daniel was a fant...|\n",
      "|      2818|16196|2009-11-04|      38623|       Graham|Daniel was great....|\n",
      "|      2818|18095|2009-11-23|      48138|        Klaus|Daniel has been m...|\n",
      "|      2818|18286|2009-11-25|      55661|      Michael|Daniels apartment...|\n",
      "|      2818|24925|2010-01-27|      33284|         Araz|Daniel was an exe...|\n",
      "|      2818|31154|2010-03-24|      82918|      Natalie|No amount of prai...|\n",
      "|      2818|35916|2010-04-16|      97200|      Ludovic|Its the perfect p...|\n",
      "|      2818|36774|2010-04-19|     103149|       Faisal|When you reach at...|\n",
      "|      2818|39856|2010-05-03|     112391|       Rickey|Daniel is a terri...|\n",
      "|      2818|45708|2010-05-24|      79479|        David|Daniel was a fant...|\n",
      "|      2818|48766|2010-06-01|      99878|Josh And Alan|Our stay with Dan...|\n",
      "|      2818|54108|2010-06-17|     131821|       Amelia|Daniel was very a...|\n",
      "+----------+-----+----------+-----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------+--------------------+\n",
      "|listing_id|      date|            comments|\n",
      "+----------+----------+--------------------+\n",
      "|      2818|2009-03-30|Daniel is really ...|\n",
      "|      2818|2009-04-24|Daniel is the mos...|\n",
      "|      2818|2009-05-03|We had such a gre...|\n",
      "|      2818|2009-05-18|Very professional...|\n",
      "|      2818|2009-05-25|Daniel is highly ...|\n",
      "|      2818|2009-06-29|Daniel was a grea...|\n",
      "|      2818|2009-07-07|Daniele is an ama...|\n",
      "|      2818|2009-09-06|You cant have a n...|\n",
      "|      2818|2009-10-01|Daniel was a fant...|\n",
      "|      2818|2009-11-04|Daniel was great....|\n",
      "|      2818|2009-11-23|Daniel has been m...|\n",
      "|      2818|2009-11-25|Daniels apartment...|\n",
      "|      2818|2010-01-27|Daniel was an exe...|\n",
      "|      2818|2010-03-24|No amount of prai...|\n",
      "|      2818|2010-04-16|Its the perfect p...|\n",
      "|      2818|2010-04-19|When you reach at...|\n",
      "|      2818|2010-05-03|Daniel is a terri...|\n",
      "|      2818|2010-05-24|Daniel was a fant...|\n",
      "|      2818|2010-06-01|Our stay with Dan...|\n",
      "|      2818|2010-06-17|Daniel was very a...|\n",
      "+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 13:19:17 WARN DFSClient: DataStreamer Exception                        \n",
      "org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n",
      "24/06/24 13:19:17 ERROR Utils: Aborting task\n",
      "com.univocity.parsers.common.TextWritingException: Error writing row.\n",
      "Internal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n",
      "\t... 17 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n",
      "24/06/24 13:19:17 ERROR FileFormatWriter: Job job_202406241319177049768596050772336_0005 aborted.\n",
      "24/06/24 13:19:17 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.\n",
      "Internal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n",
      "\t... 17 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n",
      "24/06/24 13:19:17 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (192.168.64.2 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.\n",
      "Internal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n",
      "\t... 17 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n",
      "\n",
      "24/06/24 13:19:17 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\n",
      "24/06/24 13:19:17 ERROR FileFormatWriter: Aborting job 222cb46f-ce11-4359-b3c3-84afaa6659e3.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.64.2 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.\n",
      "Internal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n",
      "\t... 17 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.\n",
      "Internal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n",
      "\t... 17 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o129.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.64.2 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.univocity.parsers.common.TextWritingException: Error writing row.\nInternal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n\t... 9 more\nCaused by: java.lang.IllegalStateException: Error closing the output.\n\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n\t... 17 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.univocity.parsers.common.TextWritingException: Error writing row.\nInternal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n\t... 9 more\nCaused by: java.lang.IllegalStateException: Error closing the output.\n\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n\t... 17 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(file_path_out, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m hdfs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:54310/user/ubuntu/ciao/reviews_details_cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:955\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    949\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m    950\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    953\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m    954\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o129.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.64.2 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.univocity.parsers.common.TextWritingException: Error writing row.\nInternal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n\t... 9 more\nCaused by: java.lang.IllegalStateException: Error closing the output.\n\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n\t... 17 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.univocity.parsers.common.TextWritingException: Error writing row.\nInternal state when error was thrown: recordCount=1982, recordData=[42970, 2014-01-27, Very lovely stay at Monicas Studio in Amsterdam. Would definitely recommend anyone visiting Amsterdam to stay there...Super clean and nicely designed  with a VERY comfy bed. It was very hard to have to leave. Had amazing sleep during the entire stay.Locationwise it is perfect as well and has a nice view of the houses opposite. You are close to everything  central or the beautiful canal area so its very convenient. Monica was very kind and helpful emailing me tips and recommendations before our stay. We couldnt have asked for more ]\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:103)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n\t... 9 more\nCaused by: java.lang.IllegalStateException: Error closing the output.\n\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n\tat com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)\n\tat com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)\n\tat com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)\n\t... 17 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/ubuntu/ciao/reviews_details_cleaned.csv/_temporary/0/_temporary/attempt_202406241319177049768596050772336_0005_m_000000_5/part-00000-d26b57ad-6e6d-4496-a815-5ee69019690b-c000.csv could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 13:57:10 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 354548 ms exceeds timeout 120000 ms\n",
      "24/06/24 13:57:10 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"listing_id\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"reviewer_id\", StringType(), True),\n",
    "    StructField(\"reviewer_name\", StringType(), True),\n",
    "    StructField(\"comments\", StringType(), True)\n",
    "])\n",
    "\n",
    "file_path = \"/home/ubuntu/DATASET/reviews_details.csv\"\n",
    "file_path_out = \"/home/ubuntu/DATASET/reviews_details_select\"\n",
    "df = spark.read.csv(file_path, header=True, schema=schema, sep=\",\", quote= '\"', escape= '\"', multiLine=True, mode=\"DROPMALFORMED\")\n",
    "for col_name in df.columns:\n",
    "        if col_name == \"date\":\n",
    "            df= df.withColumn(col_name, regexp_replace(col(col_name), \"[^0-9 -]\", \"\"))\n",
    "        else:\n",
    "            df= df.withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9 .]\", \"\"))\n",
    "df = df.filter(col(\"listing_id\").cast(\"int\").isNotNull())\n",
    "\n",
    "df.printSchema()\n",
    "#I have to limit the output because on spark it's costly!\n",
    "df.show(20)\n",
    "\n",
    "df.select(\"listing_id\", \"date\", \"comments\").show()\n",
    "df = df.select(\"listing_id\", \"date\", \"comments\")\n",
    "\n",
    "df.write.csv(file_path_out, header=False)\n",
    "\n",
    "# This generates an error because it says that no namenode is running even if\n",
    "#I run the command jps and I can see the running namenodes\n",
    "\n",
    "#hdfs_path = \"hdfs://localhost:54310/user/ubuntu/ciao/reviews_details_cleaned.csv\"\n",
    "#df.write.csv(hdfs_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a679f728-397c-46b3-9142-9b71414e6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6601a4-249b-4aa2-a7f1-c3d33d085729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
